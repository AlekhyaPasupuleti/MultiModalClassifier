{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlekhyaPasupuleti/MultiModalClassifier/blob/main/Alekhya_TFLiteexport.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2198cae",
      "metadata": {
        "id": "a2198cae"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import time\n",
        "import io\n",
        "from numpy import asarray\n",
        "from PIL import ImageOps\n",
        "#import tflite_runtime.interpreter as tflite\n",
        "#ref: https://www.tensorflow.org/lite/guide/get_started https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python\n",
        "#ref: https://github.com/lkk688/AndroidIntelligentApp/blob/main/pythonTFlite/tfliteclassify2.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d3257f",
      "metadata": {
        "id": "60d3257f"
      },
      "outputs": [],
      "source": [
        "def testtfliteexport(saved_model_dir):\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "    tflite_model = converter.convert()\n",
        "    open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d59e79",
      "metadata": {
        "id": "c0d59e79"
      },
      "outputs": [],
      "source": [
        "#https://www.tensorflow.org/lite/performance/model_optimization\n",
        "def tflitequanexport(saved_model_dir):\n",
        "    #post-training quantization quantizes weights from floating point to 8-bits of precision\n",
        "    converter_int8 = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "    converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bd1248",
      "metadata": {
        "id": "79bd1248"
      },
      "outputs": [],
      "source": [
        "   #val_ds=None\n",
        "    from TFClassifier.Datasetutil.TFdatasetutil import loadTFdataset\n",
        "    train_ds, val_ds, class_names, imageshape = loadTFdataset('fashionMNIST', 'kerasdataset', 'C:/fall2022/255/bonus_final/MultiModalClassifier/TFClassifier/outputs/fashion', 28, 28, 1)\n",
        "    def representative_data_gen():\n",
        "        for input_value, _ in val_ds.take(100):\n",
        "            yield [input_value]\n",
        "    \n",
        "    converter_int8.representative_dataset = representative_data_gen\n",
        "    #To require the converter to only output integer operations, one can specify:\n",
        "    converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] #[tf.float16]\n",
        "\n",
        "    tflite_model = converter_int8.convert()\n",
        "    tflite_model_file = 'converted_model_quant.tflite'\n",
        "\n",
        "    with open(tflite_model_file, \"wb\") as f:\n",
        "        f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7bcb7e",
      "metadata": {
        "id": "fd7bcb7e"
      },
      "outputs": [],
      "source": [
        "#to ensure compatibility with integer only devices (such as 8-bit microcontrollers) and accelerators (such as the Coral Edge TPU), you can enforce full integer quantization for all ops including the input and output, by using the following steps:\n",
        "def tflitequanintexport(saved_model_dir):\n",
        "    #post-training quantization quantizes weights from floating point to 8-bits of precision\n",
        "    converter_int8 = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "    converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5edf58c4",
      "metadata": {
        "id": "5edf58c4"
      },
      "outputs": [],
      "source": [
        "#val_ds=None\n",
        "    from TFClassifier.Datasetutil.TFdatasetutil import loadTFdataset\n",
        "    train_ds, val_ds, class_names, imageshape = loadTFdataset('fashionMNIST', 'kerasdataset', 'C:/fall2022/255/bonus_final/MultiModalClassifier/TFClassifier/outputs/fashion', 28, 28, 1)\n",
        "    def representative_data_gen():\n",
        "        for input_value, _ in val_ds.take(100):\n",
        "            yield [input_value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd53902",
      "metadata": {
        "id": "efd53902"
      },
      "outputs": [],
      "source": [
        " converter_int8.representative_dataset = representative_data_gen\n",
        "    #To require the converter to only output integer operations, one can specify:\n",
        "    converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "    #New add compared with tflitequanexport to enforce full integer quantization for all ops including the input and output\n",
        "    converter_int8.inference_input_type = tf.int8  # or tf.uint8\n",
        "    converter_int8.inference_output_type = tf.int8  # or tf.uint8\n",
        "\n",
        "    tflite_model = converter_int8.convert()\n",
        "    tflite_model_file = 'converted_model_quantint.tflite'\n",
        "\n",
        "    with open(tflite_model_file, \"wb\") as f:\n",
        "        f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd005513",
      "metadata": {
        "id": "cd005513"
      },
      "outputs": [],
      "source": [
        "def testtfliteinference(tflite_model_path):\n",
        "    # Load the TFLite model and allocate tensors.\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c9fa0c",
      "metadata": {
        "id": "e3c9fa0c"
      },
      "outputs": [],
      "source": [
        "# Get input and output tensors.\n",
        "    input_details = interpreter.get_input_details()\n",
        "    print(\"input_details\", input_details)\n",
        "    output_details = interpreter.get_output_details()\n",
        "    print(\"output_details\",output_details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5169677a",
      "metadata": {
        "id": "5169677a"
      },
      "outputs": [],
      "source": [
        " # Test the model on random input data.\n",
        "    input_shape = input_details[0]['shape']#[1, 180, 180, 3]\n",
        "\n",
        "    floating_model = input_details[0]['dtype'] == np.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef912cb3",
      "metadata": {
        "id": "ef912cb3"
      },
      "outputs": [],
      "source": [
        " #image_path='/home/lkk/Developer/MyRepo/MultiModalClassifier/tests/imgdata/sunflower.jpeg'\n",
        "    image_path='C:/fall2022/255/bonus_final/MultiModalClassifier/merry-new-year-unisex-t-shirt.jpeg'\n",
        "    img_height = input_shape[1] #180\n",
        "    img_width = input_shape[2] #180"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c805aa7",
      "metadata": {
        "id": "1c805aa7"
      },
      "outputs": [],
      "source": [
        "  if floating_model:\n",
        "        input_data=loadimage(image_path, img_height, img_width)\n",
        "    else:\n",
        "        input_data=loadimageint(image_path, img_height, img_width)\n",
        "\n",
        "    input_data = np.expand_dims(input_data, axis=3)\n",
        "    tensor_index = input_details[0]['index']\n",
        "    interpreter.set_tensor(tensor_index, input_data)\n",
        "\n",
        "    interpreter.invoke()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55b10fc",
      "metadata": {
        "id": "a55b10fc"
      },
      "outputs": [],
      "source": [
        " # The function `get_tensor()` returns a copy of the tensor data.\n",
        "    # Use `tensor()` in order to get a pointer to the tensor.\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    print(output_data)\n",
        "    output = np.squeeze(output_data) # or output_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46ae9c6",
      "metadata": {
        "id": "b46ae9c6"
      },
      "outputs": [],
      "source": [
        "  # If the model is quantized (uint8 data), then dequantize the results\n",
        "    if output_details[0]['dtype'] == np.uint8:\n",
        "        scale, zero_point = output_details[0]['quantization']\n",
        "        output = scale * (output - zero_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd321920",
      "metadata": {
        "id": "fd321920"
      },
      "outputs": [],
      "source": [
        " classindex = np.argmax(output, axis=-1)\n",
        "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    print(class_names[classindex])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81658167",
      "metadata": {
        "id": "81658167"
      },
      "outputs": [],
      "source": [
        "def loadimage(path, img_height, img_width):\n",
        "    # load image\n",
        "    image = Image.open(path).resize((img_width, img_height))\n",
        "    image = np.array(image)\n",
        "    print(np.min(image), np.max(image))#0~255\n",
        "    input=image[np.newaxis, ...]\n",
        "    input_data = np.array(input, dtype=np.float32)\n",
        "    # normalize to the range 0-1\n",
        "    input_data /= 255.0\n",
        "    print(np.min(input_data), np.max(input_data)) \n",
        "    input_data = np.expand_dims(image, axis=0)\n",
        "    return input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6528e7ef",
      "metadata": {
        "id": "6528e7ef"
      },
      "outputs": [],
      "source": [
        "def loadimageint(path, img_height, img_width):\n",
        "    # load image\n",
        "    image = Image.open(path).resize((img_width, img_height))\n",
        "    image=ImageOps.grayscale(image)\n",
        "    image = np.array(image)\n",
        "    #Convert uint8 to int8\n",
        "    image = image - 127.0\n",
        "    image = np.array(image, dtype=np.int8)\n",
        "    print(np.min(image), np.max(image))#-128 127\n",
        "    #input=image[np.newaxis, ...]\n",
        "    # add N dim\n",
        "    input_data = np.expand_dims(image, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4f6207",
      "metadata": {
        "id": "4d4f6207"
      },
      "outputs": [],
      "source": [
        "  return input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89040d00",
      "metadata": {
        "id": "89040d00"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    saved_model_dir = 'C:/fall2022/255/bonus_final/MultiModalClassifier/TFClassifier/outputs/fashion'\n",
        "    testtfliteexport(saved_model_dir)\n",
        "    tflitequanexport(saved_model_dir)\n",
        "    tflitequanintexport(saved_model_dir)\n",
        "\n",
        "    #testtfliteinference(\"converted_model_quant.tflite\")#\"converted_model.tflite\"\n",
        "    testtfliteinference(\"converted_model_quantint.tflite\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}